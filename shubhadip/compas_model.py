# -*- coding: utf-8 -*-
"""adult_census.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nHspU6fP5z9JrtjAsqbij8YIxUy9zLWq
"""

import torch
import random
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import torchvision
from torch.utils.data import Dataset
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler
import os
################# Creating training dataset class #################

class CompasData(torch.utils.data.Dataset):

  def __init__(self, X, y, scale_data=True):
    if not torch.is_tensor(X) and not torch.is_tensor(y):
      self.X = torch.from_numpy(X)
      self.y = torch.from_numpy(y)

  def __len__(self):
      return len(self.X)

  def __getitem__(self, i):
      return self.X[i], self.y[i], i



class Dataset(object):

    def __init__(self):

        data = pd.read_csv('compas.csv', na_values='?')
        scaler = StandardScaler()
        data[['Number_of_Priors']] = scaler.fit_transform(data[['Number_of_Priors']])
        X = data.drop(['Two_yr_Recidivism'],axis=1).values
        y = data['Two_yr_Recidivism'].values
        z = data['Female'].values
        
        X_train, X_test, y_train, y_test, z_train, z_test = train_test_split(X, y, z, test_size = 0.2, random_state = 42)
        

        self.trainset = CompasData(X_train, y_train, z_train)
        self.testset = CompasData(X_test, y_test, z_test)
        

    def load_data(self):

        trainloader = torch.utils.data.DataLoader(self.trainset, batch_size = 1, shuffle=False, num_workers=2)
        testloader = torch.utils.data.DataLoader(self.testset, batch_size=10, shuffle=False, num_workers=2)

        return trainloader, testloader, self.trainset


import torch.nn as nn
import torch.nn.functional as F

class Network(nn.Module):

  def __init__(self):
    super().__init__()
    self.layer1 = nn.Linear(11,64) 
    self.activ1 = nn.ReLU()
    self.layer2 = nn.Linear(64, 32)
    self.activ2 = nn.ReLU()
    self.layer3 = nn.Linear(32,2)


  def forward(self, x):
    out = self.activ1(self.layer1(x))
    out = self.activ2(self.layer2(out))
    out = self.layer3(out)

    return out

import torch.optim as optim
torch.manual_seed(123)

net = Network()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(net.parameters(), lr=0.0005, weight_decay=0.01)

ds = Dataset()
trainloader, testloader, trainset = ds.load_data()

'''
dataiter = iter(trainloader)
input, labels, i = dataiter.next()
print(type(input))
print(input)
print(labels)
print(i.shape)
'''
best_acc = 0
for epoch in range(2):
  running_loss = 0
  correct = 0
  for i, data in enumerate(trainloader):
    inputs, targets, _ = data
    
    optimizer.zero_grad()
    outputs = net(inputs.float())
    loss = criterion(outputs, targets)
    #print(loss)
    loss.backward()
    optimizer.step()

    running_loss += loss.item()
    
    _, idx = torch.max(outputs,dim=-1)

    correct += (idx == targets.squeeze()).sum()


  print(f'Epoch: {epoch} => loss: {running_loss / len(trainset):.3f}')
  running_loss = 0
  acc = 100.*correct/len(trainset)
  print("Acc:", acc)
  if acc > best_acc:
    print('Saving..')
    
    if not os.path.isdir('checkpoint_sub'):
      os.mkdir('checkpoint_sub')
    torch.save(net.state_dict(), './checkpoint_sub/compas.pth')
    best_acc = acc


net = Network()
net.load_state_dict(torch.load('./checkpoint_sub/compas.pth'))

correct = 0
total = 0
# since we're not training, we don't need to calculate the gradients for our outputs
with torch.no_grad():
    for data in testloader:
        inputs, labels, _ = data
        outputs = net(inputs.float())

        total += labels.size(0)
        _, idx = torch.max(outputs,dim=-1)
        correct += (idx == labels.squeeze()).sum()
print(f'Accuracy of the network : {100 * correct / total} %')
